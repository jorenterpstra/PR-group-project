{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Embedding, Dropout, Dense, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Bidirectional, LSTM, GRU\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBED_DIM = 300 #50, 100, 200, or 300 (see glove-6b data set)\nMIN_WORD_OCCURENCE = None #use all words\n\nNUM_HEADS = 8  # Number of attention heads\nFF_DIM = 32  # Hidden layer size in feed forward network inside transformer\nLAYER_UNITS = 64\nDENSE_DROPOUT = 0.2\n\nLR_TRANS = 0.0001\nLR_LSTM = 0.0005\nLR_GRU = 0.001\n\nBATCH_SIZE = 64\nMAX_EPOCHS = 200\n\nCALLBACK = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tokenizers import BertWordPieceTokenizer\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post', subword=False):\n    # create a tokenizer\n    if subword:\n        t = BertWordPieceTokenizer(\n            clean_text=True,\n            handle_chinese_chars=False,\n            strip_accents=False,\n            lowercase=True\n        )\n\n        t.train_from_iterator(df['text'])\n        vocab_size = t.get_vocab_size()\n        # integer encode the documents\n        encoded_list = t.encode_batch(df['text'])\n        encoded_docs = [x.ids for x in encoded_list]\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = max([len(x) for x in encoded_docs])\n    else:\n        t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n        t.fit_on_texts(df['text'])\n        vocab_size = len(t.word_index) + 1\n        # integer encode the documents\n        encoded_docs = t.texts_to_sequences(df['text'])\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length, t\n\n\ndef load_and_preprocess_data(path, max_num_words=None, pre_or_post='post', subword=False):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post, subword)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\"\npadded_docs, artists_onehot_encoded, vocab_size, max_length, token = load_and_preprocess_data(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_vector = {}\nf = open(f'/kaggle/input/glove-6b/glove.6B.{EMBED_DIM}d.txt') \nfor line in tqdm(f):\n    vector = line.split(' ')\n    word = vector[0]\n    coef = np.asarray(vector[1:],dtype = 'float32')\n    embedding_vector[word]=coef\nf.close()\n# print('Number of words found ',len(embedding_vector))\n\nembedding_matrix = np.zeros((vocab_size, EMBED_DIM))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_vectors = embedding_vector.get(word)\n    if embedding_vectors is not None:\n        embedding_matrix[i] = embedding_vector[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here use entire training set instead of validation set\nX_train, X_test, y_train, y_test = train_test_split(\n    padded_docs, artists_onehot_encoded, \n    stratify=artists_onehot_encoded, \n    test_size=0.2, random_state=42)\n\n# get validation set, which is 8% of entire data set\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, stratify=y_train,\n    test_size=0.1, random_state=42) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from https://keras.io/examples/nlp/text_classification_with_transformer/\n# original source https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.att = MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n        self.ffn = keras.Sequential(\n            [Dense(FF_DIM, activation=\"relu\"), Dense(EMBED_DIM)]\n        )\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(DENSE_DROPOUT)\n        self.dropout2 = Dropout(DENSE_DROPOUT)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformer_model():\n    with tpu_strategy.scope():\n        inputs = Input(shape=(max_length,))\n        embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                                 weights = [embedding_matrix], trainable = False)  \n\n        x = embedding_layer(inputs)\n        transformer_block = TransformerBlock()\n\n        x = transformer_block(x)\n        x = GlobalAveragePooling1D()(x)\n        x = Dropout(DENSE_DROPOUT)(x)\n\n        outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n        transformer = keras.Model(inputs=inputs, outputs=outputs)\n\n        optimizer = keras.optimizers.Adam(learning_rate=LR_TRANS)\n        transformer.compile(\n            optimizer=optimizer, loss=\"categorical_crossentropy\", \n            metrics=[\"accuracy\"], steps_per_execution=32,\n        )\n\n        return transformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lstm_model():\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n    \n        # set the input, embedding matrix uses the glove datasets\n        inputs = Input(shape=(max_length,))\n        embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                            weights = [embedding_matrix], trainable = False)    \n        x = embedding_layer(inputs)\n\n        # three LSTM layers with dropout\n        x = Bidirectional(LSTM(LAYER_UNITS, return_sequences=True))(x)\n        x = Dropout(DENSE_DROPOUT)(x)    \n        x = Bidirectional(LSTM(LAYER_UNITS, return_sequences=True))(x)\n        x = Dropout(DENSE_DROPOUT)(x)    \n        x = Bidirectional(LSTM(LAYER_UNITS))(x)\n        x = Dropout(DENSE_DROPOUT)(x)\n\n        outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n        lstm = keras.Model(inputs=inputs, outputs=outputs)\n\n        optimizer = keras.optimizers.Adam(learning_rate=LR_LSTM)\n        lstm.compile(\n            optimizer=optimizer, \n            loss=\"categorical_crossentropy\", \n            metrics=[\"accuracy\"],\n            steps_per_execution=32,\n        )\n        return lstm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gru_model():\n    # instantiating the model in the strategy scope creates the model on the TPU\n    with tpu_strategy.scope():\n        # set the input, embedding matrix uses the glove datasets\n        inputs = Input(shape=(max_length,))\n        embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                            weights = [embedding_matrix], trainable = False)    \n        x = embedding_layer(inputs)\n\n        # three GRU layers with dropout\n        x = Bidirectional(GRU(LAYER_UNITS, return_sequences=True))(x)\n        x = Dropout(DENSE_DROPOUT)(x)    \n        x = Bidirectional(GRU(LAYER_UNITS, return_sequences=True))(x)\n        x = Dropout(DENSE_DROPOUT)(x)    \n        x = Bidirectional(GRU(LAYER_UNITS))(x)\n        x = Dropout(DENSE_DROPOUT)(x)\n\n        outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n        gru = keras.Model(inputs=inputs, outputs=outputs)\n\n        optimizer = keras.optimizers.Adam(learning_rate=LR_GRU)\n\n        gru.compile(\n            optimizer=optimizer, \n            loss=\"categorical_crossentropy\", \n            metrics=[\"accuracy\"],\n            steps_per_execution=32,\n        )\n\n        return gru","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm = lstm_model()\n\nlstm.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_history = lstm.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs = MAX_EPOCHS,\n    batch_size = BATCH_SIZE,\n    callbacks = CALLBACK,\n    use_multiprocessing = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru = gru_model()\ngru.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_history = gru.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs = MAX_EPOCHS,\n    batch_size = BATCH_SIZE,\n    callbacks = CALLBACK,\n    use_multiprocessing = True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer = transformer_model()\ntransformer.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.fit(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=MAX_EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=CALLBACK,\n    use_multiprocessing=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nlstm_results = lstm.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", lstm_results)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:45:44.537901Z","iopub.execute_input":"2023-01-25T12:45:44.538940Z","iopub.status.idle":"2023-01-25T12:45:49.503539Z","shell.execute_reply.started":"2023-01-25T12:45:44.538870Z","shell.execute_reply":"2023-01-25T12:45:49.502689Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Evaluate on test data\n196/196 [==============================] - 5s 25ms/step - loss: 4.9195 - accuracy: 0.0745\ntest loss, test acc: [4.91952657699585, 0.07445956766605377]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\ngru_results = gru.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", gru_results)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:45:49.505069Z","iopub.execute_input":"2023-01-25T12:45:49.505320Z","iopub.status.idle":"2023-01-25T12:45:55.146080Z","shell.execute_reply.started":"2023-01-25T12:45:49.505292Z","shell.execute_reply":"2023-01-25T12:45:55.144918Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Evaluate on test data\n196/196 [==============================] - 6s 28ms/step - loss: 4.8601 - accuracy: 0.0804\ntest loss, test acc: [4.8601226806640625, 0.08038430660963058]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\ntransformer_results = transformer.evaluate(X_test, y_test)\nprint(\"test loss, test acc:\", transformer_results)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:45:55.147714Z","iopub.execute_input":"2023-01-25T12:45:55.147996Z","iopub.status.idle":"2023-01-25T12:46:00.144286Z","shell.execute_reply.started":"2023-01-25T12:45:55.147964Z","shell.execute_reply":"2023-01-25T12:46:00.143180Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Evaluate on test data\n196/196 [==============================] - 5s 25ms/step - loss: 4.7107 - accuracy: 0.1243\ntest loss, test acc: [4.71074914932251, 0.12425940483808517]\n","output_type":"stream"}]},{"cell_type":"code","source":"lstm_pred = np.argmax(lstm.predict(X_test), axis=1)\ngru_pred = np.argmax(gru.predict(X_test), axis=1)\ntransformer_pred = np.argmax(transformer.predict(X_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:46:00.146445Z","iopub.execute_input":"2023-01-25T12:46:00.147304Z","iopub.status.idle":"2023-01-25T12:46:34.056123Z","shell.execute_reply.started":"2023-01-25T12:46:00.147258Z","shell.execute_reply":"2023-01-25T12:46:34.055142Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"y_test = np.argmax(y_test, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:46:34.057998Z","iopub.execute_input":"2023-01-25T12:46:34.058838Z","iopub.status.idle":"2023-01-25T12:46:34.066610Z","shell.execute_reply.started":"2023-01-25T12:46:34.058783Z","shell.execute_reply":"2023-01-25T12:46:34.065391Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2023-01-25T12:47:20.156707Z","iopub.execute_input":"2023-01-25T12:47:20.157056Z","iopub.status.idle":"2023-01-25T12:47:20.164364Z","shell.execute_reply.started":"2023-01-25T12:47:20.157023Z","shell.execute_reply":"2023-01-25T12:47:20.163327Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"array([168, 211, 112, ..., 147, 180,  18])"},"metadata":{}}]},{"cell_type":"code","source":"# load pre-trained transformer results\nptt_pred = pd.read_csv(\"/kaggle/input/pre-trained-transformer/submission.csv\")['prediction']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mcnemar(prediction1, prediction2, y_test):# McNemar test to compare the results of the two models\n    CC = 0\n    CF = 0\n    FC = 0\n    FF = 0\n\n    for i in range(y_test.shape[0]):\n#         print(prediction1[i], prediction2[i], y_test[i])\n        if ((prediction1[i] == prediction2[i]) and (prediction1[i] == y_test[i])):\n            CC += 1\n        elif ((prediction1[i] != prediction2[i]) and (prediction1[i] == y_test[i])):\n            CF += 1\n        elif ((prediction1[i] != prediction2[i]) and (prediction2[i] == y_test[i])):\n            FC += 1\n        else:\n            FF +=1\n            \n    print(f\"CC: {CC}, CF: {CF}, FC: {FC}, FF: {FF}\")\n\n    mcNemar = ((CF - FC)*(CF - FC))/(CF + FC)\n    print(mcNemar)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T12:38:09.678870Z","iopub.execute_input":"2023-01-24T12:38:09.679139Z","iopub.status.idle":"2023-01-24T12:38:09.686809Z","shell.execute_reply.started":"2023-01-24T12:38:09.679111Z","shell.execute_reply":"2023-01-24T12:38:09.685855Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"mcnemar(lstm_pred, gru_pred, y_test) #corresponding p-value: 0,01837","metadata":{"execution":{"iopub.status.busy":"2023-01-24T12:41:30.489274Z","iopub.execute_input":"2023-01-24T12:41:30.490245Z","iopub.status.idle":"2023-01-24T12:41:30.508776Z","shell.execute_reply.started":"2023-01-24T12:41:30.490199Z","shell.execute_reply":"2023-01-24T12:41:30.507608Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"187 254 310 5494\n5.560283687943262\n","output_type":"stream"}]},{"cell_type":"code","source":"mcnemar(lstm_pred, transformer_pred, y_test) #corresponding p-value: 3.125e-23","metadata":{"execution":{"iopub.status.busy":"2023-01-24T12:47:01.705534Z","iopub.execute_input":"2023-01-24T12:47:01.705875Z","iopub.status.idle":"2023-01-24T12:47:01.723129Z","shell.execute_reply.started":"2023-01-24T12:47:01.705842Z","shell.execute_reply":"2023-01-24T12:47:01.722027Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"181 260 541 5263\n98.57802746566792\n","output_type":"stream"}]},{"cell_type":"code","source":"mcnemar(transformer_pred, gru_pred, y_test) #corresponding p-value: 6.925e-16","metadata":{"execution":{"iopub.status.busy":"2023-01-24T12:47:11.580382Z","iopub.execute_input":"2023-01-24T12:47:11.580986Z","iopub.status.idle":"2023-01-24T12:47:11.601516Z","shell.execute_reply.started":"2023-01-24T12:47:11.580911Z","shell.execute_reply":"2023-01-24T12:47:11.600514Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"221 501 276 5247\n65.15444015444015\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}