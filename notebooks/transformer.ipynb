{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q tensorflow==2.3.0 # Use 2.3.0 for built-in EfficientNet\n# !pip install -q git+https://github.com/keras-team/keras-tuner@master # Use github head for newly added TPU support\n# !pip install -q cloud-tpu-client # Needed for sync TPU version\n# !pip install -U tensorflow-gcs-config==2.3.0 # Needed for using private dataset","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:12:29.719960Z","iopub.execute_input":"2023-01-20T22:12:29.720473Z","iopub.status.idle":"2023-01-20T22:12:29.742216Z","shell.execute_reply.started":"2023-01-20T22:12:29.720360Z","shell.execute_reply":"2023-01-20T22:12:29.741399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this needs to be all the way at the top of file\nimport os\n# os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras_tuner\n\n# from tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Embedding, Dropout, Dense, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n\nfrom tqdm import tqdm\nimport numpy as np \n\nimport json \nimport pickle\n\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"vLsmbi__ANUM","execution":{"iopub.status.busy":"2023-01-24T08:24:58.015849Z","iopub.execute_input":"2023-01-24T08:24:58.016394Z","iopub.status.idle":"2023-01-24T08:25:04.430258Z","shell.execute_reply.started":"2023-01-24T08:24:58.016285Z","shell.execute_reply":"2023-01-24T08:25:04.428093Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"EMBED_DIM = 300 #50, 100, 200, or 300 (see glove-6b data set)\nMIN_WORD_OCCURENCE = None #use all words\n\nNUM_HEADS = 8  # Number of attention heads\nFF_DIM = 32  # Hidden layer size in feed forward network inside transformer\nLAYER_UNITS = 64\nDENSE_DROPOUT = 0.2\nLEARNING_RATES = list(np.append(np.logspace(-10, -1, num=10), np.logspace(-10, -1, num=10)*5))\n\nMAX_TRIALS = 5\nBATCH_SIZE = 64\nMAX_EPOCHS = 200\n\nCALLBACK = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:12:36.036456Z","iopub.execute_input":"2023-01-20T22:12:36.039283Z","iopub.status.idle":"2023-01-20T22:12:36.057854Z","shell.execute_reply.started":"2023-01-20T22:12:36.039244Z","shell.execute_reply":"2023-01-20T22:12:36.056941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TPU does not work with keras_tuner \nCan only be made to work if a google cloud storage account is made, but then you need to pay. \n\nCommented out all the TPU stuff for tuning.\n","metadata":{}},{"cell_type":"code","source":"# TPU_ADDRESS = os.environ['TPU_NAME']\n# print(\"TPU address:\", TPU_ADDRESS)\n# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n# tf.config.experimental_connect_to_cluster(resolver)\n# tf.tpu.experimental.initialize_tpu_system(resolver)\n# strategy = tf.distribute.experimental.TPUStrategy(resolver)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:12:36.062096Z","iopub.execute_input":"2023-01-20T22:12:36.066284Z","iopub.status.idle":"2023-01-20T22:12:36.073051Z","shell.execute_reply.started":"2023-01-20T22:12:36.065652Z","shell.execute_reply":"2023-01-20T22:12:36.072059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessin\nIt's cleanest to just import this from the `preprocesing.py`file, but this is annoying in Kaggle. So therefore it's pasted here.","metadata":{}},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tokenizers import BertWordPieceTokenizer\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post', subword=False):\n    # create a tokenizer\n    if subword:\n        t = BertWordPieceTokenizer(\n            clean_text=True,\n            handle_chinese_chars=False,\n            strip_accents=False,\n            lowercase=True\n        )\n\n        t.train_from_iterator(df['text'])\n        vocab_size = t.get_vocab_size()\n        # integer encode the documents\n        encoded_list = t.encode_batch(df['text'])\n        encoded_docs = [x.ids for x in encoded_list]\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = max([len(x) for x in encoded_docs])\n    else:\n        t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n        t.fit_on_texts(df['text'])\n        vocab_size = len(t.word_index) + 1\n        # integer encode the documents\n        encoded_docs = t.texts_to_sequences(df['text'])\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length, t\n\n\ndef load_and_preprocess_data(path, max_num_words=None, pre_or_post='post', subword=False):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post, subword)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-20T22:12:36.079750Z","iopub.execute_input":"2023-01-20T22:12:36.080148Z","iopub.status.idle":"2023-01-20T22:12:36.201670Z","shell.execute_reply.started":"2023-01-20T22:12:36.080113Z","shell.execute_reply":"2023-01-20T22:12:36.200525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_vector = {}\nf = open(f'/kaggle/input/glove-6b/glove.6B.{EMBED_DIM}d.txt') \nfor line in tqdm(f):\n    vector = line.split(' ')\n    word = vector[0]\n    coef = np.asarray(vector[1:],dtype = 'float32')\n    embedding_vector[word]=coef\nf.close()\nprint('Number of words found ',len(embedding_vector))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:12:36.206322Z","iopub.execute_input":"2023-01-20T22:12:36.207247Z","iopub.status.idle":"2023-01-20T22:13:03.671083Z","shell.execute_reply.started":"2023-01-20T22:12:36.207207Z","shell.execute_reply":"2023-01-20T22:13:03.670047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\"\npadded_docs, artists_onehot_encoded, vocab_size, max_length, token = load_and_preprocess_data(path, max_num_words=None, subword=False)","metadata":{"id":"euqN6GNjtHrd","execution":{"iopub.status.busy":"2023-01-20T22:13:03.672622Z","iopub.execute_input":"2023-01-20T22:13:03.673242Z","iopub.status.idle":"2023-01-20T22:13:14.692004Z","shell.execute_reply.started":"2023-01-20T22:13:03.673202Z","shell.execute_reply":"2023-01-20T22:13:14.691013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBED_DIM))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_vectors = embedding_vector.get(word)\n    if embedding_vectors is not None:\n        embedding_matrix[i] = embedding_vector[word]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:13:14.693628Z","iopub.execute_input":"2023-01-20T22:13:14.693976Z","iopub.status.idle":"2023-01-20T22:13:14.895659Z","shell.execute_reply.started":"2023-01-20T22:13:14.693942Z","shell.execute_reply":"2023-01-20T22:13:14.894560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_docs, artists_onehot_encoded, \n    stratify=artists_onehot_encoded, \n    test_size=0.2, random_state=42)\n\n# get validation set, which is 8% of entire data set\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, stratify=y_train,\n    test_size=0.1, random_state=42) ","metadata":{"id":"ZR-dFIxxtNWr","execution":{"iopub.status.busy":"2023-01-20T22:13:14.897041Z","iopub.execute_input":"2023-01-20T22:13:14.897518Z","iopub.status.idle":"2023-01-20T22:13:23.351027Z","shell.execute_reply.started":"2023-01-20T22:13:14.897480Z","shell.execute_reply":"2023-01-20T22:13:23.350021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from https://keras.io/examples/nlp/text_classification_with_transformer/\n# original source https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n        self.att = MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)\n        self.ffn = keras.Sequential(\n            [Dense(FF_DIM, activation=\"relu\"), Dense(EMBED_DIM)]\n        )\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(DENSE_DROPOUT)\n        self.dropout2 = Dropout(DENSE_DROPOUT)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"lBocKngIHPJs","execution":{"iopub.status.busy":"2023-01-20T22:13:23.352554Z","iopub.execute_input":"2023-01-20T22:13:23.352922Z","iopub.status.idle":"2023-01-20T22:13:23.361313Z","shell.execute_reply.started":"2023-01-20T22:13:23.352886Z","shell.execute_reply":"2023-01-20T22:13:23.360191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformer_model(hp):\n#     with tpu_strategy.scope():\n    inputs = Input(shape=(max_length,))\n    embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                             weights = [embedding_matrix], trainable = False)  \n\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock()\n\n    x = transformer_block(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(DENSE_DROPOUT)(x)\n\n    outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n    transformer = keras.Model(inputs=inputs, outputs=outputs)\n\n    lr = hp.Choice('learning_rate', LEARNING_RATES)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    transformer.compile(\n        optimizer=optimizer, loss=\"categorical_crossentropy\", \n        metrics=[\"accuracy\"], steps_per_execution=32, #potentially steps_per_execution raises an error\n    )\n\n    return transformer","metadata":{"id":"4sMfqAuUHkWg","execution":{"iopub.status.busy":"2023-01-20T22:13:23.363162Z","iopub.execute_input":"2023-01-20T22:13:23.363881Z","iopub.status.idle":"2023-01-20T22:13:23.373402Z","shell.execute_reply.started":"2023-01-20T22:13:23.363846Z","shell.execute_reply":"2023-01-20T22:13:23.372408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\n\ntuner = keras_tuner.BayesianOptimization(\n    hypermodel=transformer_model,\n    objective=\"val_accuracy\",\n    max_trials=MAX_TRIALS,\n    seed=42,\n    directory=\"tuning_results\",\n    project_name=\"transformer\",\n#     distribution_strategy=strategy, # only when tpu is used\n)\n\ntuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:13:23.374838Z","iopub.execute_input":"2023-01-20T22:13:23.375200Z","iopub.status.idle":"2023-01-20T22:13:26.469294Z","shell.execute_reply.started":"2023-01-20T22:13:23.375167Z","shell.execute_reply":"2023-01-20T22:13:26.468260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner.search(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=MAX_EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=CALLBACK,\n    use_multiprocessing=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T22:13:26.470676Z","iopub.execute_input":"2023-01-20T22:13:26.471302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get best model's hyper parameters\ntuner.get_best_hyperparameters(num_trials=1)\nprint('\\n')\ntuner.results_summary(num_trials=MAX_TRIALS)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}