{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:01.714292Z","iopub.execute_input":"2023-01-24T11:43:01.714665Z","iopub.status.idle":"2023-01-24T11:43:01.813104Z","shell.execute_reply.started":"2023-01-24T11:43:01.714578Z","shell.execute_reply":"2023-01-24T11:43:01.812099Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:01.816505Z","iopub.execute_input":"2023-01-24T11:43:01.817080Z","iopub.status.idle":"2023-01-24T11:43:06.467587Z","shell.execute_reply.started":"2023-01-24T11:43:01.817053Z","shell.execute_reply":"2023-01-24T11:43:06.466819Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"There are 1 GPU(s) available.\nDevice name: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import AutoTokenizer\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post', subword=False):\n    # create a tokenizer\n    if subword == 'subword':\n        t = BertWordPieceTokenizer(\n            clean_text=True,\n            handle_chinese_chars=False,\n            strip_accents=False,\n            lowercase=True\n        )\n\n        t.train_from_iterator(df['text'])\n        vocab_size = t.get_vocab_size()\n        # integer encode the documents\n        encoded_list = t.encode_batch(df['text'])\n        encoded_docs = [x.ids for x in encoded_list]\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = max([len(x) for x in encoded_docs])\n        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n    elif subword == 'word':\n        t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n        t.fit_on_texts(df['text'])\n        vocab_size = len(t.word_index) + 1\n        # integer encode the documents\n        encoded_docs = t.texts_to_sequences(df['text'])\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n    else:\n        t = AutoTokenizer.from_pretrained('bert-base-uncased')\n        padded_docs = t(df['text'].tolist(), padding=True, truncation=True, return_tensors='np')\n\n\n\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    #onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, integer_encoded, t, label_encoder\n\n\ndef load_and_preprocess_data(path, max_num_words=None, pre_or_post='post', subword=False):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post, subword)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:06.469214Z","iopub.execute_input":"2023-01-24T11:43:06.469716Z","iopub.status.idle":"2023-01-24T11:43:12.357186Z","shell.execute_reply.started":"2023-01-24T11:43:06.469678Z","shell.execute_reply":"2023-01-24T11:43:12.356461Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2023-01-24 11:43:07.131952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"path = \"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\"\npadded_docs, onehot_encoded, token, label = load_and_preprocess_data(path)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:12.359536Z","iopub.execute_input":"2023-01-24T11:43:12.360071Z","iopub.status.idle":"2023-01-24T11:43:49.395961Z","shell.execute_reply.started":"2023-01-24T11:43:12.360037Z","shell.execute_reply":"2023-01-24T11:43:49.395232Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a11383b9db0402a91c690865bd09b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c81f904612d14b618e9a039c536880c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff9168ce28a745a09cbe87a3528de43d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a64570c2aa9432aa2e580f2a7a3cc5f"}},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ninput_ids = padded_docs['input_ids']\nattention_masks = padded_docs['attention_mask']\n\nNUM_LABELS = max(onehot_encoded) + 1\n\nX_train, X_test, y_train, y_test, mask_train, mask_test = train_test_split(\n    input_ids, onehot_encoded, attention_masks, test_size=0.2, \n    random_state=42, stratify=onehot_encoded)\n\n# get validation set, which is 8% of entire data set\nX_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n    X_train, y_train, mask_train, stratify=y_train,\n    test_size=0.1, random_state=42)\n\nX_train = torch.tensor(X_train)\nmask_train = torch.tensor(mask_train)\ny_train = torch.tensor(y_train, dtype=torch.long)\n\nX_val = torch.tensor(X_val)\nmask_val = torch.tensor(mask_val)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\nX_test = torch.tensor(X_test)\nmask_test = torch.tensor(mask_test)\ny_test = torch.tensor(y_test, dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.398998Z","iopub.execute_input":"2023-01-24T11:43:49.399286Z","iopub.status.idle":"2023-01-24T11:43:49.663951Z","shell.execute_reply.started":"2023-01-24T11:43:49.399251Z","shell.execute_reply":"2023-01-24T11:43:49.663155Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(NUM_LABELS)\nprint(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.665407Z","iopub.execute_input":"2023-01-24T11:43:49.665672Z","iopub.status.idle":"2023-01-24T11:43:49.685235Z","shell.execute_reply.started":"2023-01-24T11:43:49.665639Z","shell.execute_reply":"2023-01-24T11:43:49.684499Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"268\ntensor([  2, 134, 206,  ..., 243, 108, 155])\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(X_train, mask_train, y_train)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(X_val, mask_val, y_val)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our test set\ntest_data = TensorDataset(X_test, mask_test, y_test)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.686318Z","iopub.execute_input":"2023-01-24T11:43:49.686714Z","iopub.status.idle":"2023-01-24T11:43:49.693054Z","shell.execute_reply.started":"2023-01-24T11:43:49.686681Z","shell.execute_reply":"2023-01-24T11:43:49.692120Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.694310Z","iopub.execute_input":"2023-01-24T11:43:49.694785Z","iopub.status.idle":"2023-01-24T11:43:49.702859Z","shell.execute_reply.started":"2023-01-24T11:43:49.694753Z","shell.execute_reply":"2023-01-24T11:43:49.702048Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"703\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, num_labels, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, num_labels, num_labels\n        \n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            #nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.704114Z","iopub.execute_input":"2023-01-24T11:43:49.704573Z","iopub.status.idle":"2023-01-24T11:43:49.723527Z","shell.execute_reply.started":"2023-01-24T11:43:49.704541Z","shell.execute_reply":"2023-01-24T11:43:49.722671Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 4.44 ms, sys: 42 µs, total: 4.49 ms\nWall time: 5.69 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(lr, eps, epochs=4, freeze_bert=False):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(NUM_LABELS, freeze_bert)\n    \n    #print(torch.cuda.memory_summary(device=None, abbreviated=False))\n    # Tell PyTorch to run the model on GPU\n    report_gpu()\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=lr,    # Default learning rate\n                      eps=eps    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.726037Z","iopub.execute_input":"2023-01-24T11:43:49.726418Z","iopub.status.idle":"2023-01-24T11:43:49.738525Z","shell.execute_reply.started":"2023-01-24T11:43:49.726385Z","shell.execute_reply":"2023-01-24T11:43:49.737774Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, optimizer, scheduler, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    \n    if evaluation:\n        outputs = {'train_loss': [],'train_acc': [], 'val_loss': [], 'val_acc': [] }\n    else:\n        outputs = {'train_loss': [],'train_acc': []}\n    \n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Train accuracy':^14} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n        \n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts, total_acc, batch_acc = 0, 0, 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            report_gpu()\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n            \n            report_gpu()\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n            preds = torch.argmax(logits, dim=1).flatten()\n                \n            cur_acc = (preds == b_labels).cpu().numpy().mean() * 100\n            batch_acc += cur_acc\n            total_acc += cur_acc\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {total_acc / step:^14.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts, batch_acc = 0, 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n        avg_train_acc = total_acc / len(train_dataloader)\n        \n        outputs['train_loss'].append(avg_train_loss)\n        outputs['train_acc'].append(avg_train_acc)\n        \n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n            outputs['val_loss'].append(val_loss)\n            outputs['val_acc'].append(val_accuracy)\n\n            \n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f}  | {avg_train_acc:^14.6f}| {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n    torch.save(model, '/kaggle/working/BERT-model')\n    return outputs\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy\n","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.741316Z","iopub.execute_input":"2023-01-24T11:43:49.741513Z","iopub.status.idle":"2023-01-24T11:43:49.761639Z","shell.execute_reply.started":"2023-01-24T11:43:49.741489Z","shell.execute_reply":"2023-01-24T11:43:49.760482Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import gc\ndef report_gpu():\n   #print(torch.cuda.list_gpu_processes())\n   gc.collect()\n   torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.763114Z","iopub.execute_input":"2023-01-24T11:43:49.763708Z","iopub.status.idle":"2023-01-24T11:43:49.773033Z","shell.execute_reply.started":"2023-01-24T11:43:49.763674Z","shell.execute_reply":"2023-01-24T11:43:49.772283Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"set_seed(42)    # Set seed for reproducibility\nEPOCHS = 50\nlr = 1e-4\neps = 1e-6\nbert_classifier, optimizer, scheduler = initialize_model(lr, eps, epochs=EPOCHS, freeze_bert=True)\nhistory = train(bert_classifier, optimizer, scheduler, train_dataloader, val_dataloader, \n                epochs=EPOCHS, evaluation=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:43:49.774430Z","iopub.execute_input":"2023-01-24T11:43:49.774800Z","iopub.status.idle":"2023-01-24T11:47:30.341949Z","shell.execute_reply.started":"2023-01-24T11:43:49.774768Z","shell.execute_reply":"2023-01-24T11:47:30.339716Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171d0f10b9fb48948b3a035483d66f3d"}},"metadata":{}},{"name":"stdout","text":"Start training...\n\n Epoch  |  Batch  |  Train Loss  | Train accuracy |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   1    |   20    |   5.594500   |    0.468750    |     -      |     -     |   20.02  \n   1    |   40    |   5.589546   |    0.468750    |     -      |     -     |   18.99  \n   1    |   60    |   5.607351   |    0.364583    |     -      |     -     |   19.02  \n   1    |   80    |   5.582361   |    0.429688    |     -      |     -     |   18.86  \n   1    |   100   |   5.589366   |    0.468750    |     -      |     -     |   18.93  \n   1    |   120   |   5.580254   |    0.625000    |     -      |     -     |   18.91  \n   1    |   140   |   5.583315   |    0.580357    |     -      |     -     |   18.86  \n   1    |   160   |   5.590578   |    0.546875    |     -      |     -     |   18.84  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/181538901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_bert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m history = train(bert_classifier, optimizer, scheduler, train_dataloader, val_dataloader, \n\u001b[0;32m----> 7\u001b[0;31m                 epochs=EPOCHS, evaluation=True)\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_18/294359093.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mbatch_counts\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Load batch to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mreport_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/206938976.py\u001b[0m in \u001b[0;36mreport_gpu\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreport_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;31m#print(torch.cuda.list_gpu_processes())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m    \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2023-01-24T11:47:30.344458Z","iopub.status.idle":"2023-01-24T11:47:30.346759Z","shell.execute_reply.started":"2023-01-24T11:47:30.346500Z","shell.execute_reply":"2023-01-24T11:47:30.346528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}