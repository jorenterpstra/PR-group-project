{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow import keras\nimport keras_tuner\nfrom keras.layers import Input, Embedding, Bidirectional, Dense, Dropout, LSTM, GRU\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:28:32.568651Z","iopub.execute_input":"2023-01-20T15:28:32.569114Z","iopub.status.idle":"2023-01-20T15:28:38.015074Z","shell.execute_reply.started":"2023-01-20T15:28:32.569025Z","shell.execute_reply":"2023-01-20T15:28:38.014087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBED_DIM = 300 #50, 100, 200, or 300 (see glove-6b data set)\nMIN_WORD_OCCURENCE = None # use all words\n\nLAYER_UNITS = 64\nDENSE_DROPOUT = 0.2\n# LEARNING_RATES = list(np.append(np.logspace(-10, -1, num=10), np.logspace(-10, -1, num=10)*5))\nLEARNING_RATES = [0.0007, 0.0009, 0.001, 0.002, 0.003, 0.004]\n\nMAX_TRIALS = 10\nBATCH_SIZE = 64\nMAX_EPOCHS = 200\n\nCALLBACK = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:28:38.017065Z","iopub.execute_input":"2023-01-20T15:28:38.017784Z","iopub.status.idle":"2023-01-20T15:28:38.025113Z","shell.execute_reply.started":"2023-01-20T15:28:38.017746Z","shell.execute_reply":"2023-01-20T15:28:38.023854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tokenizers import BertWordPieceTokenizer\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post', subword=False):\n    # create a tokenizer\n    if subword:\n        t = BertWordPieceTokenizer(\n            clean_text=True,\n            handle_chinese_chars=False,\n            strip_accents=False,\n            lowercase=True\n        )\n\n        t.train_from_iterator(df['text'])\n        vocab_size = t.get_vocab_size()\n        # integer encode the documents\n        encoded_list = t.encode_batch(df['text'])\n        encoded_docs = [x.ids for x in encoded_list]\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = max([len(x) for x in encoded_docs])\n    else:\n        t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n        t.fit_on_texts(df['text'])\n        vocab_size = len(t.word_index) + 1\n        # integer encode the documents\n        encoded_docs = t.texts_to_sequences(df['text'])\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length, t\n\n\ndef load_and_preprocess_data(path, max_num_words=None, pre_or_post='post', subword=False):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post, subword)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:28:38.026543Z","iopub.execute_input":"2023-01-20T15:28:38.027182Z","iopub.status.idle":"2023-01-20T15:28:38.118875Z","shell.execute_reply.started":"2023-01-20T15:28:38.027146Z","shell.execute_reply":"2023-01-20T15:28:38.118039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\"\npadded_docs, artists_onehot_encoded, vocab_size, max_length, token = load_and_preprocess_data(path)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:28:38.122056Z","iopub.execute_input":"2023-01-20T15:28:38.122414Z","iopub.status.idle":"2023-01-20T15:28:49.525125Z","shell.execute_reply.started":"2023-01-20T15:28:38.122379Z","shell.execute_reply":"2023-01-20T15:28:49.524156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_vector = {}\nf = open(f'/kaggle/input/glove-6b/glove.6B.{EMBED_DIM}d.txt') \nfor line in tqdm(f):\n    vector = line.split(' ')\n    word = vector[0]\n    coef = np.asarray(vector[1:],dtype = 'float32')\n    embedding_vector[word]=coef\nf.close()\n# print('Number of words found ',len(embedding_vector))\n\nembedding_matrix = np.zeros((vocab_size, EMBED_DIM))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_vectors = embedding_vector.get(word)\n    if embedding_vectors is not None:\n        embedding_matrix[i] = embedding_vector[word]","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:28:49.526596Z","iopub.execute_input":"2023-01-20T15:28:49.527065Z","iopub.status.idle":"2023-01-20T15:29:15.251117Z","shell.execute_reply.started":"2023-01-20T15:28:49.527028Z","shell.execute_reply":"2023-01-20T15:29:15.250022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_docs, artists_onehot_encoded, \n    stratify=artists_onehot_encoded, \n    test_size=0.2, random_state=42)\n\n# get validation set, which is 8% of entire data set\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, stratify=y_train,\n    test_size=0.1, random_state=42) ","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:29:15.252607Z","iopub.execute_input":"2023-01-20T15:29:15.253269Z","iopub.status.idle":"2023-01-20T15:29:24.533538Z","shell.execute_reply.started":"2023-01-20T15:29:15.253225Z","shell.execute_reply":"2023-01-20T15:29:24.532449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lstm_model(hp):\n    # set the input, embedding matrix uses the glove datasets\n    inputs = Input(shape=(max_length,))\n    embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                        weights = [embedding_matrix], trainable = False)    \n    x = embedding_layer(inputs)\n    \n    # three LSTM layers with dropout\n    x = Bidirectional(LSTM(LAYER_UNITS, return_sequences=True))(x)\n    x = Dropout(DENSE_DROPOUT)(x)    \n    x = Bidirectional(LSTM(LAYER_UNITS, return_sequences=True))(x)\n    x = Dropout(DENSE_DROPOUT)(x)    \n    x = Bidirectional(LSTM(LAYER_UNITS))(x)\n    x = Dropout(DENSE_DROPOUT)(x)\n    \n    outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n    \n    lstm = keras.Model(inputs=inputs, outputs=outputs)\n\n    lr = hp.Choice('learning_rate', LEARNING_RATES)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    lstm.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    return lstm","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:29:24.534922Z","iopub.execute_input":"2023-01-20T15:29:24.535278Z","iopub.status.idle":"2023-01-20T15:29:24.543418Z","shell.execute_reply.started":"2023-01-20T15:29:24.535244Z","shell.execute_reply":"2023-01-20T15:29:24.542342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gru_model(hp):\n    # set the input, embedding matrix uses the glove datasets\n    inputs = Input(shape=(max_length,))\n    embedding_layer = Embedding(vocab_size, EMBED_DIM, input_length=max_length, \n                        weights = [embedding_matrix], trainable = False)    \n    x = embedding_layer(inputs)\n    \n    # three GRU layers with dropout\n    x = Bidirectional(GRU(LAYER_UNITS, return_sequences=True))(x)\n    x = Dropout(DENSE_DROPOUT)(x)    \n    x = Bidirectional(GRU(LAYER_UNITS, return_sequences=True))(x)\n    x = Dropout(DENSE_DROPOUT)(x)    \n    x = Bidirectional(GRU(LAYER_UNITS))(x)\n    x = Dropout(DENSE_DROPOUT)(x)\n    \n    outputs = Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n    \n    gru = keras.Model(inputs=inputs, outputs=outputs)\n\n    lr = hp.Choice('learning_rate', LEARNING_RATES)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n                      \n    gru.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    return gru","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:29:24.545090Z","iopub.execute_input":"2023-01-20T15:29:24.545729Z","iopub.status.idle":"2023-01-20T15:29:24.555846Z","shell.execute_reply.started":"2023-01-20T15:29:24.545692Z","shell.execute_reply":"2023-01-20T15:29:24.554792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lstm_tuner = keras_tuner.BayesianOptimization(\n#     hypermodel=lstm_model,\n#     objective=\"val_accuracy\",\n#     max_trials=MAX_TRIALS,\n#     seed=42,\n#     directory=\"tuner_results\",\n#     project_name=\"lstm\"\n# )\n\ngru_tuner = keras_tuner.BayesianOptimization(\n    hypermodel=gru_model,\n    objective=\"val_accuracy\",\n    max_trials=MAX_TRIALS,\n    seed=42,\n    directory=\"tuner_results\",\n    project_name=\"gru\"\n)\n\n# lstm_tuner.search_space_summary()\nprint('\\n')\ngru_tuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:29:24.557300Z","iopub.execute_input":"2023-01-20T15:29:24.557689Z","iopub.status.idle":"2023-01-20T15:29:30.101813Z","shell.execute_reply.started":"2023-01-20T15:29:24.557656Z","shell.execute_reply":"2023-01-20T15:29:30.100790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lstm_tuner.search(\n#     X_train,\n#     y_train,\n#     validation_data=(X_val, y_val),\n#     epochs=MAX_EPOCHS,\n#     batch_size=BATCH_SIZE,\n#     callbacks=CALLBACK,\n#     use_multiprocessing=True,\n# )","metadata":{"execution":{"iopub.status.busy":"2023-01-20T15:29:30.105327Z","iopub.execute_input":"2023-01-20T15:29:30.105653Z","iopub.status.idle":"2023-01-20T20:00:38.074726Z","shell.execute_reply.started":"2023-01-20T15:29:30.105618Z","shell.execute_reply":"2023-01-20T20:00:38.073658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get best model's hyper parameters\n# lstm_tuner.get_best_hyperparameters(num_trials=1)\n# print('\\n')\n# lstm_tuner.results_summary(num_trials=MAX_TRIALS)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:00:38.076235Z","iopub.execute_input":"2023-01-20T20:00:38.076856Z","iopub.status.idle":"2023-01-20T20:00:38.083701Z","shell.execute_reply.started":"2023-01-20T20:00:38.076818Z","shell.execute_reply":"2023-01-20T20:00:38.082418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gru_tuner.search(\n    X_train,\n    y_train,\n    validation_data=(X_val, y_val),\n    epochs=MAX_EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=CALLBACK,\n    use_multiprocessing=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T20:00:38.085221Z","iopub.execute_input":"2023-01-20T20:00:38.085867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get best model's hyper parameters\ngru_tuner.get_best_hyperparameters(num_trials=1)\nprint('\\n')\ngru_tuner.results_summary(num_trials=MAX_TRIALS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}