{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"f6e4305676b138d1b7f009a5810f93093a69501b82ebd2fa06fb8bc39f340d64"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n\nimport tensorflow as tf\nfrom tensorflow import keras\n# import keras_nlp\n# # recommended by https://keras.io/guides/keras_nlp/getting_started/\n# # Use mixed precision for optimal performance\n# keras.mixed_precision.set_global_policy(\"mixed_float16\")\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\nfrom tensorflow.keras.models import Sequential\n\n\nimport json \nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nembed_dim = 300 #50, 100, 200, or 300\nmin_word_occurence = 0","metadata":{"id":"vLsmbi__ANUM","execution":{"iopub.status.busy":"2023-01-17T22:33:35.670564Z","iopub.execute_input":"2023-01-17T22:33:35.671071Z","iopub.status.idle":"2023-01-17T22:33:42.889374Z","shell.execute_reply.started":"2023-01-17T22:33:35.670952Z","shell.execute_reply":"2023-01-17T22:33:42.888056Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2023-01-17 22:33:36.416671: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-17 22:33:36.416815: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","output_type":"stream"}]},{"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T22:33:42.891254Z","iopub.execute_input":"2023-01-17T22:33:42.891529Z","iopub.status.idle":"2023-01-17T22:33:48.425942Z","shell.execute_reply.started":"2023-01-17T22:33:42.891492Z","shell.execute_reply":"2023-01-17T22:33:48.424848Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2023-01-17 22:33:42.899463: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n2023-01-17 22:33:42.899514: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n2023-01-17 22:33:42.899537: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (8f424cab3d6f): /proc/driver/nvidia/version does not exist\n2023-01-17 22:33:42.902482: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-17 22:33:42.924561: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2249995000 Hz\n2023-01-17 22:33:42.925114: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ca17aed760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2023-01-17 22:33:42.925148: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2023-01-17 22:33:42.970238: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-17 22:33:42.970308: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30019}\n2023-01-17 22:33:42.990637: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.2:8470}\n2023-01-17 22:33:42.990691: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:30019}\n2023-01-17 22:33:42.992476: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:30019\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nfrom tokenizers import BertWordPieceTokenizer\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post', subword=False):\n    # create a tokenizer\n    if subword:\n        t = BertWordPieceTokenizer(\n            clean_text=True,\n            handle_chinese_chars=False,\n            strip_accents=False,\n            lowercase=True\n        )\n\n        t.train_from_iterator(df['text'])\n        vocab_size = t.get_vocab_size()\n        # integer encode the documents\n        encoded_list = t.encode_batch(df['text'])\n        encoded_docs = [x.ids for x in encoded_list]\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = max([len(x) for x in encoded_docs])\n    else:\n        t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n        t.fit_on_texts(df['text'])\n        vocab_size = len(t.word_index) + 1\n        # integer encode the documents\n        encoded_docs = t.texts_to_sequences(df['text'])\n        # pad documents to be as long as the longest sequence in the dataset\n        max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length, t\n\n\ndef load_and_preprocess_data(path, max_num_words=None, pre_or_post='post', subword=False):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post, subword)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-17T22:33:48.427246Z","iopub.execute_input":"2023-01-17T22:33:48.427468Z","iopub.status.idle":"2023-01-17T22:33:48.521296Z","shell.execute_reply.started":"2023-01-17T22:33:48.427439Z","shell.execute_reply":"2023-01-17T22:33:48.520078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nembedding_vector = {}\nf = open(f'/kaggle/input/glove-6b/glove.6B.{embed_dim}d.txt') \nfor line in tqdm(f):\n    vector = line.split(' ')\n    word = vector[0]\n    coef = np.asarray(vector[1:],dtype = 'float32')\n    embedding_vector[word]=coef\nf.close()\nprint('Number of words found ',len(embedding_vector))","metadata":{"execution":{"iopub.status.busy":"2023-01-17T22:33:48.524069Z","iopub.execute_input":"2023-01-17T22:33:48.524467Z","iopub.status.idle":"2023-01-17T22:34:15.368657Z","shell.execute_reply.started":"2023-01-17T22:33:48.524423Z","shell.execute_reply":"2023-01-17T22:34:15.367377Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"400000it [00:26, 14910.14it/s]","output_type":"stream"},{"name":"stdout","text":"Number of words found  400000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"padded_docs, artists_onehot_encoded, vocab_size, max_length, token = load_and_preprocess_data(\"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\", max_num_words=None, subword=False)","metadata":{"id":"euqN6GNjtHrd","execution":{"iopub.status.busy":"2023-01-17T22:34:15.370055Z","iopub.execute_input":"2023-01-17T22:34:15.370352Z","iopub.status.idle":"2023-01-17T22:34:27.532407Z","shell.execute_reply.started":"2023-01-17T22:34:15.370322Z","shell.execute_reply":"2023-01-17T22:34:27.531851Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, embed_dim))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_vectors = embedding_vector.get(word)\n    if embedding_vectors is not None:\n        embedding_matrix[i] = embedding_vector[word]","metadata":{"execution":{"iopub.status.busy":"2023-01-17T22:34:27.533364Z","iopub.execute_input":"2023-01-17T22:34:27.534009Z","iopub.status.idle":"2023-01-17T22:34:27.691716Z","shell.execute_reply.started":"2023-01-17T22:34:27.533976Z","shell.execute_reply":"2023-01-17T22:34:27.691080Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 61310/61310 [00:00<00:00, 407540.53it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_docs, artists_onehot_encoded, \n    stratify=artists_onehot_encoded, \n    test_size=0.2, random_state=42)","metadata":{"id":"ZR-dFIxxtNWr","execution":{"iopub.status.busy":"2023-01-17T22:34:27.692655Z","iopub.execute_input":"2023-01-17T22:34:27.692874Z","iopub.status.idle":"2023-01-17T22:34:31.784302Z","shell.execute_reply.started":"2023-01-17T22:34:27.692851Z","shell.execute_reply":"2023-01-17T22:34:31.783144Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# from https://keras.io/examples/nlp/text_classification_with_transformer/\n# original source https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"lBocKngIHPJs","execution":{"iopub.status.busy":"2023-01-17T22:34:31.785670Z","iopub.execute_input":"2023-01-17T22:34:31.785886Z","iopub.status.idle":"2023-01-17T22:34:31.793644Z","shell.execute_reply.started":"2023-01-17T22:34:31.785858Z","shell.execute_reply":"2023-01-17T22:34:31.792786Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"num_heads = 8  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    inputs = layers.Input(shape=(max_length,))\n#     embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_length, \n#                             weights = [embedding_matrix], trainable = False)    \n    embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_length, trainable = True)\n    x = embedding_layer(inputs)\n    x = layers.Dropout(0.25)(x)\n    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n    x = transformer_block(x)\n#     x = layers.Dense(ff_dim, activation=\"relu\")(x)\n#     x = layers.Dense(embed_dim)(x)\n#     transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n#     x = transformer_block2(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.1)(x)\n#     x = layers.Dense(32)(x)\n#     x = layers.Dropout(0.1)(x)\n\n    outputs = layers.Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n    multi_transformer_layers = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"id":"4sMfqAuUHkWg","execution":{"iopub.status.busy":"2023-01-17T22:34:31.795375Z","iopub.execute_input":"2023-01-17T22:34:31.795599Z","iopub.status.idle":"2023-01-17T22:34:33.059747Z","shell.execute_reply.started":"2023-01-17T22:34:31.795559Z","shell.execute_reply":"2023-01-17T22:34:33.058705Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with tpu_strategy.scope():\n    inputs = layers.Input(shape=(max_length,))\n    embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_length, \n                            weights = [embedding_matrix], trainable = False)    \n#     embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_length, trainable = True)\n    x = embedding_layer(inputs)\n    x = Dropout(0.25)(x)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = Dropout(0.25, seed=42)(x)\n    x = Bidirectional(LSTM(128))(x)\n    outputs = layers.Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n    lstm = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"execution":{"iopub.status.busy":"2023-01-17T22:34:33.061724Z","iopub.execute_input":"2023-01-17T22:34:33.061970Z","iopub.status.idle":"2023-01-17T22:34:35.939613Z","shell.execute_reply.started":"2023-01-17T22:34:33.061939Z","shell.execute_reply":"2023-01-17T22:34:35.938463Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# # Model params.\n# NUM_LAYERS = 3\n# MODEL_DIM = 256\n# INTERMEDIATE_DIM = 512\n# NUM_HEADS = 4\n# DROPOUT = 0.1\n# NORM_EPSILON = 1e-5\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n# # with tpu_strategy.scope():\n# inputs = layers.Input(shape=(max_length,))\n# # embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n# embedding_layer = Embedding(vocab_size, embed_dim, input_length=max_length, \n#                         weights = [embedding_matrix], trainable = False)\n# x = embedding_layer(inputs)\n\n# # Apply layer normalization and dropout to the embedding.\n# x = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(x)\n# x = keras.layers.Dropout(rate=DROPOUT)(x)\n\n# # Add a number of encoder blocks\n# for i in range(NUM_LAYERS):\n#     x = keras_nlp.layers.TransformerEncoder(\n#         intermediate_dim=INTERMEDIATE_DIM,\n#         num_heads=NUM_HEADS,\n#         dropout=DROPOUT,\n#         layer_norm_epsilon=NORM_EPSILON,\n#     )(x)\n\n# outputs = layers.Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\n# multi_transformer_layers = keras.Model(inputs, outputs)\n# multi_transformer_layers.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-17T21:36:28.352235Z","iopub.execute_input":"2023-01-17T21:36:28.352462Z","iopub.status.idle":"2023-01-17T21:36:28.356869Z","shell.execute_reply.started":"2023-01-17T21:36:28.352436Z","shell.execute_reply":"2023-01-17T21:36:28.356119Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# optimizer = keras.optimizers.Adam(learning_rate=0.00005)\n# multi_transformer_layers.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n# multi_transformer_layers.summary()\n\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nlstm.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nlstm.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRds1gbvI_ZO","outputId":"9597e566-92ae-427e-d637-82c1b58d60f6","execution":{"iopub.status.busy":"2023-01-17T22:34:35.940866Z","iopub.execute_input":"2023-01-17T22:34:35.941774Z","iopub.status.idle":"2023-01-17T22:34:35.983292Z","shell.execute_reply.started":"2023-01-17T22:34:35.941660Z","shell.execute_reply":"2023-01-17T22:34:35.982026Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 950)]             0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 950, 300)          18393300  \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 950, 300)          0         \n_________________________________________________________________\nbidirectional (Bidirectional (None, 950, 256)          439296    \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 950, 256)          0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 256)               394240    \n_________________________________________________________________\ndense_3 (Dense)              (None, 268)               68876     \n=================================================================\nTotal params: 19,295,712\nTrainable params: 902,412\nNon-trainable params: 18,393,300\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]\n\n# history = multi_transformer_layers.fit(\n#     X_train,\n#     y_train,\n#     validation_split=0.1,\n#     epochs=200,\n#     batch_size=64,\n#     callbacks=callbacks,\n#     use_multiprocessing=True\n# )\n\nhistory = lstm.fit(\n    X_train,\n    y_train,\n    validation_split=0.1,\n    epochs=200,\n    batch_size=64,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"v3NDgYEkGK2L","outputId":"502d2579-b2ca-4bb2-d2f8-d390bf11a8c5","execution":{"iopub.status.busy":"2023-01-17T22:34:35.984806Z","iopub.execute_input":"2023-01-17T22:34:35.985056Z","iopub.status.idle":"2023-01-17T22:53:11.996245Z","shell.execute_reply.started":"2023-01-17T22:34:35.985028Z","shell.execute_reply":"2023-01-17T22:53:11.994856Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/200\n352/352 [==============================] - 47s 111ms/step - loss: 5.5680 - accuracy: 0.0065 - val_loss: 5.5167 - val_accuracy: 0.0084\nEpoch 2/200\n352/352 [==============================] - 34s 96ms/step - loss: 5.4349 - accuracy: 0.0146 - val_loss: 5.3113 - val_accuracy: 0.0272\nEpoch 3/200\n352/352 [==============================] - 34s 97ms/step - loss: 5.2429 - accuracy: 0.0273 - val_loss: 5.2067 - val_accuracy: 0.0308\nEpoch 4/200\n352/352 [==============================] - 34s 97ms/step - loss: 5.0803 - accuracy: 0.0400 - val_loss: 5.0767 - val_accuracy: 0.0360\nEpoch 5/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.9390 - accuracy: 0.0481 - val_loss: 5.0711 - val_accuracy: 0.0376\nEpoch 6/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.8951 - accuracy: 0.0540 - val_loss: 4.9144 - val_accuracy: 0.0520\nEpoch 7/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.7007 - accuracy: 0.0717 - val_loss: 4.8500 - val_accuracy: 0.0592\nEpoch 8/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.6298 - accuracy: 0.0803 - val_loss: 4.7869 - val_accuracy: 0.0604\nEpoch 9/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.4753 - accuracy: 0.0928 - val_loss: 4.7616 - val_accuracy: 0.0749\nEpoch 10/200\n352/352 [==============================] - 34s 98ms/step - loss: 4.3904 - accuracy: 0.1004 - val_loss: 4.7554 - val_accuracy: 0.0701\nEpoch 11/200\n352/352 [==============================] - 35s 98ms/step - loss: 4.3586 - accuracy: 0.1044 - val_loss: 4.6976 - val_accuracy: 0.0785\nEpoch 12/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.2076 - accuracy: 0.1282 - val_loss: 4.6958 - val_accuracy: 0.0777\nEpoch 13/200\n352/352 [==============================] - 34s 97ms/step - loss: 4.0926 - accuracy: 0.1363 - val_loss: 4.7141 - val_accuracy: 0.0765\nEpoch 14/200\n352/352 [==============================] - 34s 98ms/step - loss: 3.9807 - accuracy: 0.1566 - val_loss: 4.7015 - val_accuracy: 0.0837\nEpoch 15/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.8685 - accuracy: 0.1711 - val_loss: 4.7063 - val_accuracy: 0.0821\nEpoch 16/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.7742 - accuracy: 0.1878 - val_loss: 4.6968 - val_accuracy: 0.0877\nEpoch 17/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.6992 - accuracy: 0.1995 - val_loss: 4.7004 - val_accuracy: 0.0869\nEpoch 18/200\n352/352 [==============================] - 34s 98ms/step - loss: 3.5937 - accuracy: 0.2138 - val_loss: 4.7414 - val_accuracy: 0.0805\nEpoch 19/200\n352/352 [==============================] - 34s 98ms/step - loss: 3.4855 - accuracy: 0.2344 - val_loss: 4.7490 - val_accuracy: 0.0893\nEpoch 20/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.3979 - accuracy: 0.2456 - val_loss: 4.8213 - val_accuracy: 0.0877\nEpoch 21/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.3088 - accuracy: 0.2608 - val_loss: 4.8007 - val_accuracy: 0.0889\nEpoch 22/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.2098 - accuracy: 0.2792 - val_loss: 4.8472 - val_accuracy: 0.0925\nEpoch 23/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.1066 - accuracy: 0.2988 - val_loss: 4.8744 - val_accuracy: 0.0869\nEpoch 24/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.0077 - accuracy: 0.3207 - val_loss: 4.9072 - val_accuracy: 0.0917\nEpoch 25/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.9516 - accuracy: 0.3273 - val_loss: 5.0026 - val_accuracy: 0.0757\nEpoch 26/200\n352/352 [==============================] - 34s 97ms/step - loss: 3.0936 - accuracy: 0.3002 - val_loss: 4.9765 - val_accuracy: 0.0801\nEpoch 27/200\n352/352 [==============================] - 34s 98ms/step - loss: 2.8511 - accuracy: 0.3454 - val_loss: 5.0109 - val_accuracy: 0.0777\nEpoch 28/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.7369 - accuracy: 0.3675 - val_loss: 5.0370 - val_accuracy: 0.0833\nEpoch 29/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.6517 - accuracy: 0.3883 - val_loss: 5.1214 - val_accuracy: 0.0857\nEpoch 30/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.5529 - accuracy: 0.4007 - val_loss: 5.1560 - val_accuracy: 0.0829\nEpoch 31/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.4716 - accuracy: 0.4186 - val_loss: 5.2040 - val_accuracy: 0.0793\nEpoch 32/200\n352/352 [==============================] - 34s 97ms/step - loss: 2.4132 - accuracy: 0.4263 - val_loss: 5.2288 - val_accuracy: 0.0813\nRestoring model weights from the end of the best epoch.\nEpoch 00032: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"# tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n\n# # Get the optimal hyperparameters\n# best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# print(f\"\"\"\n# The hyperparameter search is complete. The optimal number of units in the first densely-connected\n# layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n# is {best_hps.get('learning_rate')}.\n# \"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-01-17T20:29:37.726342Z","iopub.status.idle":"2023-01-17T20:29:37.727130Z","shell.execute_reply.started":"2023-01-17T20:29:37.726819Z","shell.execute_reply":"2023-01-17T20:29:37.726848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name = \"transformerv3.1_subword=True\"\nwith open(file_name + \"_history\", 'w') as f:\n    json.dump(history.history, f)","metadata":{"id":"VeAaC2IqtP5O","execution":{"iopub.status.busy":"2023-01-17T20:29:37.728677Z","iopub.status.idle":"2023-01-17T20:29:37.729380Z","shell.execute_reply.started":"2023-01-17T20:29:37.729097Z","shell.execute_reply":"2023-01-17T20:29:37.729123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_graphs(history, string, title):\n    if type(history) is dict:\n        plt.plot(history[string])\n        plt.plot(history['val_'+string])\n    else:\n        plt.plot(history.history[string])\n        plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.title(f\"{title} {string}\")\n    plt.show()\n    \ndef plot_model_comparison(history1, history2, string, title):\n    plt.plot(history1.history[string])\n    plt.plot(history1.history['val_'+string])\n    plt.plot(history2.history[string])\n    plt.plot(history2.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.title(f\"{title} {string}\")\n    plt.show()\n    \nplot_graphs(history, 'accuracy', 'transformer')\nplot_graphs(history, 'loss', 'transformer')\n\n# plot_graphs(trainable_history, 'accuracy', 'trainable embedding GRU')\n# plot_graphs(trainable_history, 'loss', 'trainable embedding GRU')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/transformerv2.0_history', \"rb\") as file_pi:\n    abc = pickle.load(file_pi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(abc, 'accuracy', 'transformer')\nplot_graphs(abc, 'loss', 'transformer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.to_json()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(abc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}