{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"f6e4305676b138d1b7f009a5810f93093a69501b82ebd2fa06fb8bc39f340d64"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"vLsmbi__ANUM","execution":{"iopub.status.busy":"2023-01-10T16:17:03.800854Z","iopub.execute_input":"2023-01-10T16:17:03.801733Z","iopub.status.idle":"2023-01-10T16:17:14.795394Z","shell.execute_reply.started":"2023-01-10T16:17:03.801642Z","shell.execute_reply":"2023-01-10T16:17:14.794398Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras_preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df):\n    t = Tokenizer()\n    t.fit_on_texts(df['text'])\n    # keep only words that appear more than min_df times\n    # keep only words that appear more than min_df times\n    word_docs = {word: freq for word, freq in t.word_docs.items() if freq > 5}\n\n    # create new tokenizer\n    t_filtered = Tokenizer()\n\n    # fit t_filtered\n    t_filtered.word_index = {word: i + 1 for i, (word, freq) in enumerate(word_docs.items())}\n    t_filtered.word_counts = word_docs\n\n    vocab_size = len(t_filtered.word_index) + 1\n    # integer encode the documents\n    encoded_docs = t_filtered.texts_to_sequences(df['text'])\n    # pad documents to be as long as the longest sequence in the dataset\n    max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length\n\ndef load_and_preprocess_data(path, in_colab=False):\n    \"\"\"\n    Load the data and preprocess it, expect runtime of 20 seconds.\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n      \n    # remove artist with fewer than 30 songs\n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df)\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-10T16:35:39.860832Z","iopub.execute_input":"2023-01-10T16:35:39.861192Z","iopub.status.idle":"2023-01-10T16:35:39.875705Z","shell.execute_reply.started":"2023-01-10T16:35:39.861160Z","shell.execute_reply":"2023-01-10T16:35:39.874487Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"padded_docs, artists_onehot_encoded, vocab_size, max_length = load_and_preprocess_data(\"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\")","metadata":{"id":"euqN6GNjtHrd","execution":{"iopub.status.busy":"2023-01-10T16:35:41.202386Z","iopub.execute_input":"2023-01-10T16:35:41.202781Z","iopub.status.idle":"2023-01-10T16:36:38.644017Z","shell.execute_reply.started":"2023-01-10T16:35:41.202750Z","shell.execute_reply":"2023-01-10T16:36:38.642974Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded_docs, artists_onehot_encoded, test_size=0.2, random_state=42)","metadata":{"id":"ZR-dFIxxtNWr","execution":{"iopub.status.busy":"2023-01-10T16:36:38.646310Z","iopub.execute_input":"2023-01-10T16:36:38.646722Z","iopub.status.idle":"2023-01-10T16:36:38.708015Z","shell.execute_reply.started":"2023-01-10T16:36:38.646686Z","shell.execute_reply":"2023-01-10T16:36:38.706877Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"lBocKngIHPJs","execution":{"iopub.status.busy":"2023-01-10T16:36:38.709571Z","iopub.execute_input":"2023-01-10T16:36:38.710259Z","iopub.status.idle":"2023-01-10T16:36:38.722357Z","shell.execute_reply.started":"2023-01-10T16:36:38.710215Z","shell.execute_reply":"2023-01-10T16:36:38.721293Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, max_length, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=max_length, output_dim=embed_dim)\n\n    def call(self, x):\n        max_length = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=max_length, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"id":"HqxCRgM2Hf8F","execution":{"iopub.status.busy":"2023-01-10T16:36:38.725708Z","iopub.execute_input":"2023-01-10T16:36:38.726080Z","iopub.status.idle":"2023-01-10T16:36:38.733492Z","shell.execute_reply.started":"2023-01-10T16:36:38.726043Z","shell.execute_reply":"2023-01-10T16:36:38.732356Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(max_length,))\nembedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\ntransformer = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"id":"4sMfqAuUHkWg","execution":{"iopub.status.busy":"2023-01-10T16:36:38.735065Z","iopub.execute_input":"2023-01-10T16:36:38.735420Z","iopub.status.idle":"2023-01-10T16:36:38.898319Z","shell.execute_reply.started":"2023-01-10T16:36:38.735385Z","shell.execute_reply":"2023-01-10T16:36:38.897437Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"transformer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\ntransformer.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRds1gbvI_ZO","outputId":"9597e566-92ae-427e-d637-82c1b58d60f6","execution":{"iopub.status.busy":"2023-01-10T16:36:38.899391Z","iopub.execute_input":"2023-01-10T16:36:38.899732Z","iopub.status.idle":"2023-01-10T16:36:38.913541Z","shell.execute_reply.started":"2023-01-10T16:36:38.899695Z","shell.execute_reply":"2023-01-10T16:36:38.912337Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 950)]             0         \n_________________________________________________________________\ntoken_and_position_embedding (None, 950, 32)           491968    \n_________________________________________________________________\ntransformer_block_1 (Transfo (None, 950, 32)           10656     \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 32)                0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 268)               8844      \n=================================================================\nTotal params: 512,524\nTrainable params: 512,524\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]\n\nhistory = transformer.fit(\n    X_train,\n    y_train,\n    validation_split=0.1,\n    epochs=200,\n    batch_size=64,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"v3NDgYEkGK2L","outputId":"502d2579-b2ca-4bb2-d2f8-d390bf11a8c5","execution":{"iopub.status.busy":"2023-01-10T16:36:38.915367Z","iopub.execute_input":"2023-01-10T16:36:38.915728Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/200\n352/352 [==============================] - 27s 73ms/step - loss: 5.5880 - accuracy: 0.0044 - val_loss: 5.5824 - val_accuracy: 0.0044\nEpoch 2/200\n352/352 [==============================] - 25s 72ms/step - loss: 5.4851 - accuracy: 0.0085 - val_loss: 5.4044 - val_accuracy: 0.0076\nEpoch 3/200\n352/352 [==============================] - 26s 73ms/step - loss: 5.3608 - accuracy: 0.0120 - val_loss: 5.3329 - val_accuracy: 0.0132\nEpoch 4/200\n352/352 [==============================] - 26s 73ms/step - loss: 5.2967 - accuracy: 0.0137 - val_loss: 5.3101 - val_accuracy: 0.0172\nEpoch 5/200\n327/352 [==========================>...] - ETA: 1s - loss: 5.2444 - accuracy: 0.0159","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"VeAaC2IqtP5O"},"execution_count":null,"outputs":[]}]}