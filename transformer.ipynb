{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"f6e4305676b138d1b7f009a5810f93093a69501b82ebd2fa06fb8bc39f340d64"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding\n\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"vLsmbi__ANUM","execution":{"iopub.status.busy":"2023-01-13T13:17:27.152558Z","iopub.execute_input":"2023-01-13T13:17:27.152997Z","iopub.status.idle":"2023-01-13T13:17:27.163123Z","shell.execute_reply.started":"2023-01-13T13:17:27.152960Z","shell.execute_reply":"2023-01-13T13:17:27.162245Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"import string\nimport regex as re\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n\ndef preprocess(text):\n    # Remove integers\n    text = re.sub(r'\\d+', '', text)\n\n    # remove newlines as \\r and \\n\n    text = re.sub(r'\\r', '', text)\n\n    # Convert to lowercase\n    text = text.lower()\n\n    # Remove punctuation marks\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n\n    return text\n\n\ndef encode_text_and_labels(df, max_num_words, pre_or_post='post'):\n    \n    # create a tokenizer\n    t = Tokenizer(num_words=max_num_words, oov_token='<unk>')\n    t.fit_on_texts(df['text'])\n    vocab_size = len(t.word_index) + 1\n    # integer encode the documents\n    encoded_docs = t.texts_to_sequences(df['text'])\n    # pad documents to be as long as the longest sequence in the dataset\n    max_length = df['text'].apply(lambda x: len(x.split(' '))).max()\n    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=pre_or_post)\n\n    # integer encode\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(df['artist'])\n    # binary encode\n    onehot_encoded = to_categorical(integer_encoded)\n    return padded_docs, onehot_encoded, vocab_size, max_length, t\n\n\ndef load_and_preprocess_data(path, max_num_words, pre_or_post='post'):\n    \"\"\"\n    Load the data and preprocess it\n    :param path: path to the data\n    :return: preprocessed data in the form of a pandas dataframe. The first item returned is the data,\n    the second is the labels, the third is the vocabulary size, and the fourth is the maximum length of a sequence\n    \"\"\"\n    df = pd.read_csv(path)\n    \n    df = df.groupby('artist').filter(lambda x: len(x) > 100)\n\n    df['text'] = df['text'].apply(preprocess)\n\n    # Identify the rows that contain duplicated text in the 'song' column\n    no_covers = ~df['song'].duplicated()\n\n    # Filter the DataFrame to include only the rows with unique text\n    df = df[no_covers]\n\n    # prepare text data for a recurrent network\n    return encode_text_and_labels(df, max_num_words, pre_or_post='post')\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-01-13T13:15:13.208428Z","iopub.execute_input":"2023-01-13T13:15:13.208791Z","iopub.status.idle":"2023-01-13T13:15:13.221551Z","shell.execute_reply.started":"2023-01-13T13:15:13.208760Z","shell.execute_reply":"2023-01-13T13:15:13.220567Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nembedding_vector = {}\nf = open('/kaggle/input/glove-6b/glove.6B.100d.txt') \nfor line in tqdm(f):\n    vector = line.split(' ')\n    word = vector[0]\n    coef = np.asarray(vector[1:],dtype = 'float32')\n    embedding_vector[word]=coef\nf.close()\nprint('Number of words found ',len(embedding_vector))","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:15:22.638135Z","iopub.execute_input":"2023-01-13T13:15:22.639026Z","iopub.status.idle":"2023-01-13T13:15:31.234303Z","shell.execute_reply.started":"2023-01-13T13:15:22.638971Z","shell.execute_reply":"2023-01-13T13:15:31.233329Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"400000it [00:08, 47358.12it/s]","output_type":"stream"},{"name":"stdout","text":"Number of words found  400000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"padded_docs, artists_onehot_encoded, vocab_size, max_length, token = load_and_preprocess_data(\"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\", None)","metadata":{"id":"euqN6GNjtHrd","execution":{"iopub.status.busy":"2023-01-13T13:16:22.255513Z","iopub.execute_input":"2023-01-13T13:16:22.255871Z","iopub.status.idle":"2023-01-13T13:16:32.268948Z","shell.execute_reply.started":"2023-01-13T13:16:22.255840Z","shell.execute_reply":"2023-01-13T13:16:32.267962Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size,100))\nfor word,i in tqdm(token.word_index.items()):\n    embedding_vectors = embedding_vector.get(word)\n    if embedding_vectors is not None:\n        embedding_matrix[i] = embedding_vector[word]","metadata":{"execution":{"iopub.status.busy":"2023-01-13T13:16:32.270861Z","iopub.execute_input":"2023-01-13T13:16:32.271462Z","iopub.status.idle":"2023-01-13T13:16:32.412107Z","shell.execute_reply.started":"2023-01-13T13:16:32.271426Z","shell.execute_reply":"2023-01-13T13:16:32.411172Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 61310/61310 [00:00<00:00, 472132.77it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(padded_docs, artists_onehot_encoded, test_size=0.2, random_state=42)","metadata":{"id":"ZR-dFIxxtNWr","execution":{"iopub.status.busy":"2023-01-13T13:16:57.251085Z","iopub.execute_input":"2023-01-13T13:16:57.251715Z","iopub.status.idle":"2023-01-13T13:16:57.313928Z","shell.execute_reply.started":"2023-01-13T13:16:57.251673Z","shell.execute_reply":"2023-01-13T13:16:57.312822Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"id":"lBocKngIHPJs","execution":{"iopub.status.busy":"2023-01-13T13:17:41.186720Z","iopub.execute_input":"2023-01-13T13:17:41.187422Z","iopub.status.idle":"2023-01-13T13:17:41.198256Z","shell.execute_reply.started":"2023-01-13T13:17:41.187377Z","shell.execute_reply":"2023-01-13T13:17:41.197056Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# class TokenAndPositionEmbedding(layers.Layer):\n#     def __init__(self, max_length, vocab_size, embed_dim):\n#         super().__init__()\n#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n#         self.pos_emb = layers.Embedding(input_dim=max_length, output_dim=embed_dim)\n\n#     def call(self, x):\n#         max_length = tf.shape(x)[-1]\n#         positions = tf.range(start=0, limit=max_length, delta=1)\n#         positions = self.pos_emb(positions)\n#         x = self.token_emb(x)\n#         return x + positions","metadata":{"id":"HqxCRgM2Hf8F","execution":{"iopub.status.busy":"2023-01-10T16:36:38.725708Z","iopub.execute_input":"2023-01-10T16:36:38.726080Z","iopub.status.idle":"2023-01-10T16:36:38.733492Z","shell.execute_reply.started":"2023-01-10T16:36:38.726043Z","shell.execute_reply":"2023-01-10T16:36:38.732356Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"embed_dim = 100  # Embedding size for each token\nnum_heads = 6  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(max_length,))\n# embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\nembedding_layer = Embedding(vocab_size, 100, input_length=max_length, \n                        weights = [embedding_matrix], trainable = False)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(32, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(artists_onehot_encoded.shape[1], activation=\"softmax\")(x)\n\ntransformer = keras.Model(inputs=inputs, outputs=outputs)","metadata":{"id":"4sMfqAuUHkWg","execution":{"iopub.status.busy":"2023-01-13T13:19:20.936146Z","iopub.execute_input":"2023-01-13T13:19:20.937129Z","iopub.status.idle":"2023-01-13T13:19:21.098214Z","shell.execute_reply.started":"2023-01-13T13:19:20.937093Z","shell.execute_reply":"2023-01-13T13:19:21.097299Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"transformer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\ntransformer.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRds1gbvI_ZO","outputId":"9597e566-92ae-427e-d637-82c1b58d60f6","execution":{"iopub.status.busy":"2023-01-13T13:19:25.936238Z","iopub.execute_input":"2023-01-13T13:19:25.937161Z","iopub.status.idle":"2023-01-13T13:19:25.954514Z","shell.execute_reply.started":"2023-01-13T13:19:25.937126Z","shell.execute_reply":"2023-01-13T13:19:25.953220Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 950)]             0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 950, 100)          6131100   \n_________________________________________________________________\ntransformer_block_1 (Transfo (None, 950, 100)          248832    \n_________________________________________________________________\nglobal_average_pooling1d (Gl (None, 100)               0         \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 100)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 32)                3232      \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 268)               8844      \n=================================================================\nTotal params: 6,392,008\nTrainable params: 260,908\nNon-trainable params: 6,131,100\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"callbacks = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5,\n                                                    restore_best_weights=True,\n                                                    verbose=1)]\n\nhistory = transformer.fit(\n    X_train,\n    y_train,\n    validation_split=0.1,\n    epochs=200,\n    batch_size=64,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"v3NDgYEkGK2L","outputId":"502d2579-b2ca-4bb2-d2f8-d390bf11a8c5","execution":{"iopub.status.busy":"2023-01-13T13:19:32.218432Z","iopub.execute_input":"2023-01-13T13:19:32.218798Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2023-01-13 13:19:32.375980: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/200\n352/352 [==============================] - 104s 288ms/step - loss: 5.5863 - accuracy: 0.0051 - val_loss: 5.5689 - val_accuracy: 0.0048\nEpoch 2/200\n108/352 [========>.....................] - ETA: 1:10 - loss: 5.5301 - accuracy: 0.0081","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"VeAaC2IqtP5O"},"execution_count":null,"outputs":[]}]}